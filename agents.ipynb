{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Agents",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_XUchWfDUiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnDHYctcxHyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjncR5Z7xElk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class env:\n",
        "    def __init__(self, size, step_limit, debug=0):\n",
        "        self.number_of_resources = 3\n",
        "        self.amount_of_resource = 4\n",
        "        self.debug = debug\n",
        "        self.size = size\n",
        "        self.step_limit = step_limit\n",
        "        #self.valid_moves = ['up', 'down', 'left', 'right', 'pass']\n",
        "        self.valid_moves = [0, 1, 2, 3, 4]\n",
        "        self.move_map = {0:[-1,0], \n",
        "                         1:[1,0], \n",
        "                         2:[0,-1], \n",
        "                         3:[0,1], \n",
        "                         4:[0,0]}\n",
        "        \n",
        "        self.initialize_map()\n",
        "        self.initialize_resources()\n",
        "        \n",
        "\n",
        "    \n",
        "    def initialize_map(self):\n",
        "        self.map = np.zeros((self.size, self.size), dtype=int)\n",
        "        agent_rand_loc = random.randint(0,1)\n",
        "        if agent_rand_loc:\n",
        "            agent_1_coords = [0,self.size -1]\n",
        "            self.map[0,self.size -1 ] = -1\n",
        "            agent_2_coords = [0,0]\n",
        "            self.map[0, 0] = -2 \n",
        "            \n",
        "        else:\n",
        "            agent_1_coords = [0,0]\n",
        "            self.map[0,0] = -1\n",
        "            agent_2_coords = [0,self.size-1]\n",
        "            self.map[0,self.size-1] = -2\n",
        "        self.agents_coords = [agent_1_coords, agent_2_coords]\n",
        "        \n",
        "    def initialize_resources(self):\n",
        "        self.resources = []\n",
        "        index = [i for i in range(self.size**2) if (i != 0) and (i != self.size-1)]\n",
        "        all_indexes = random.sample(index, self.number_of_resources*self.amount_of_resource)\n",
        "        res_ind = []\n",
        "        for i in range(self.number_of_resources):\n",
        "            resource = random.sample(all_indexes, self.amount_of_resource)\n",
        "            all_indexes = [r for r in all_indexes if r not in resource]\n",
        "            resource = [(int(r/self.size),r%self.size) for r in resource]\n",
        "            empty_map = np.zeros((self.size, self.size), dtype=int)\n",
        "            for ind in range(len(resource)):\n",
        "                x, y = resource[ind][0], resource[ind][1]                \n",
        "                self.map[x, y] = i+1\n",
        "                empty_map[x, y] = 1\n",
        "            self.resources.append(empty_map)\n",
        "        goal_index = [i for i in range(self.number_of_resources)]\n",
        "        goals = random.sample(goal_index, 2)\n",
        "        self.agents_goals = goals\n",
        "        self.agents_res_collected = [[], []]\n",
        "        self.agents_self_goal_collected = [0, 0]\n",
        "        self.agents_othr_goal_collected = [0, 0]\n",
        "        self.agents_nthr_goal_collected = [0, 0]\n",
        "                \n",
        "    def step(self, agent_numb, move_code):\n",
        "        if self.step_limit == 0:\n",
        "            if self.debug:\n",
        "                print('step limit reached, error')\n",
        "        else:\n",
        "            self.step_limit -= 1\n",
        "            if self.debug:\n",
        "                print(self.step_limit, 'steps left')\n",
        "        if move_code not in self.valid_moves:\n",
        "            if self.debug:\n",
        "                print('icorrect move')\n",
        "        else:\n",
        "            agent_old_coords = self.agents_coords[agent_numb]\n",
        "            move = self.move_map[move_code]\n",
        "            agent_new_coords = l3 = [l+r for l,r in zip(agent_old_coords, move)]\n",
        "            correct_move = True\n",
        "            for coord in agent_new_coords:\n",
        "                if (coord < 0) or (coord >= self.size):\n",
        "                    if self.debug:\n",
        "                        print('icorrect move')\n",
        "                    correct_move = False\n",
        "            other_coords = self.agents_coords[1-agent_numb]\n",
        "            if agent_new_coords == other_coords:\n",
        "                if self.debug:\n",
        "                    print('icorrect move')\n",
        "                correct_move = False\n",
        "            if correct_move:\n",
        "                self.map[agent_old_coords[0],agent_old_coords[1]] = 0\n",
        "                self.map[agent_new_coords[0],agent_new_coords[1]] = -1 - agent_numb\n",
        "                self.agents_coords[agent_numb] = agent_new_coords\n",
        "                for res in range(self.number_of_resources):\n",
        "                    if self.resources[res][agent_new_coords[0],agent_new_coords[1]] != 0:\n",
        "                        self.agents_res_collected[agent_numb].append(res)\n",
        "                        self.resources[res][agent_new_coords[0],agent_new_coords[1]] = 0\n",
        "                        if res == self.agents_goals[agent_numb]:\n",
        "                            self.agents_self_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected self goal resource:', res)\n",
        "                        elif res == self.agents_goals[1 - agent_numb]:\n",
        "                            self.agents_othr_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected other goal resource:', res)\n",
        "                        else:\n",
        "                            self.agents_nthr_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected neither goal resource:', res)\n",
        "                            \n",
        "    def observation(self, agent_numb):\n",
        "        feature_vec = []\n",
        "        self_other_coords = [self.agents_coords[agent_numb], \n",
        "                             self.agents_coords[1 - agent_numb] \n",
        "                            ]\n",
        "        for agnt_coords in self_other_coords:\n",
        "            agent_map = np.zeros((self.size, self.size), dtype=int)\n",
        "            agent_map[agnt_coords[0],agnt_coords[1]] = 1\n",
        "            agent_map = np.reshape(agent_map, self.size*self.size)\n",
        "            feature_vec.extend(agent_map)\n",
        "            \n",
        "        for res in range(len(self.resources)):\n",
        "            feature_vec.extend(np.reshape(self.resources[res], self.size*self.size))\n",
        "        \n",
        "        return feature_vec\n",
        "    \n",
        "    def reward(self):\n",
        "        \n",
        "        n_self_c_self = self.agents_self_goal_collected[0]\n",
        "        n_othr_c_self = self.agents_othr_goal_collected[1]\n",
        "        \n",
        "        n_self_c_othr = self.agents_othr_goal_collected[0]\n",
        "        n_othr_c_othr = self.agents_self_goal_collected[1]\n",
        "        \n",
        "        n_self_c_nthr = self.agents_nthr_goal_collected[0]\n",
        "        n_othr_c_nthr = self.agents_nthr_goal_collected[1]\n",
        "        \n",
        "        reward = (n_self_c_self + n_othr_c_self)**2 + \\\n",
        "                 (n_self_c_othr + n_othr_c_othr)**2 - \\\n",
        "                 (n_self_c_nthr + n_othr_c_nthr)**2 \n",
        "        \n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_limit = step_limit\n",
        "        self.initialize_map()\n",
        "        self.initialize_resources()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZDDGyqWDXGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ffunction(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(Ffunction, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.lstm = nn.LSTM(input_size, 64)\n",
        "    self.fullycon1 = nn.Linear(64, 64)\n",
        "    nn.init.orthogonal_(self.fullycon1.weight)\n",
        "    self.fullycon2 = nn.Linear(64, 64)\n",
        "    nn.init.orthogonal_(self.fullycon2.weight)\n",
        "    self.policy = nn.Linear(64, 5)\n",
        "    nn.init.orthogonal_(self.policy.weight)\n",
        "    self.value = nn.Linear(64, 1)\n",
        "    nn.init.orthogonal_(self.value.weight)\n",
        "\n",
        "    self.beta = 0.01\n",
        "\n",
        "  def forward(self, x): #x = input\n",
        "    result, _ = self.lstm(x.view(-1, 1, self.input_size))\n",
        "    result = nn.functional.elu(self.fullycon1(result.view(-1, 64)))\n",
        "    result = nn.functional.elu(self.fullycon2(result))\n",
        "    return nn.functional.softmax(self.policy(result)), self.value(result)\n",
        "\n",
        "  def get_a2c_loss(self, actions, states, rewards):\n",
        "    probs, values = self.forward(states)\n",
        "    advantage = rewards - values.view(batch_size)\n",
        "    actions_one_hot = torch.zeros(batch_size, 5)\n",
        "    actions_one_hot[torch.arange(batch_size), actions.view(batch_size)] = 1\n",
        "    logs = torch.log(torch.sum(probs.view(batch_size, 5) * actions_one_hot.view(batch_size, 5), (1)))\n",
        "    #print(values)\n",
        "    logloss = logs * (advantage.view(batch_size)).detach()\n",
        "    entropyloss = -torch.sum(probs * torch.log(probs), (1))\n",
        "    #print(logloss)\n",
        "\n",
        "    return logloss + self.beta * entropyloss + 0.5 * advantage * advantage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNPqy0rLEEVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent1 = Ffunction(326)\n",
        "opt1 = torch.optim.Adam(agent1.parameters(), betas=(0.9, 0.999), eps=1 * 10 ** -8)\n",
        "batch_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FKJ_uhnxrgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent2 = Ffunction(326)\n",
        "opt2 = torch.optim.Adam(agent2.parameters(), betas=(0.9, 0.999), eps=1 * 10 ** -8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYepYxj6yHTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# buffer_ag_N = [[input, action, revard]*number_of_samples]\n",
        "buffer_ag_1 = []\n",
        "buffer_ag_2 = []\n",
        "\n",
        "def sample_from_buffer(buffer, batch_size=10):\n",
        "    batch = random.sample(buffer, batch_size)\n",
        "    inputs = [i[0] for i in batch]\n",
        "    actions = [i[1] for i in batch]\n",
        "    revards = [i[2] for i in batch]\n",
        "    \n",
        "    inputs = torch.FloatTensor(inputs)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    revards = torch.FloatTensor(revards)\n",
        "    \n",
        "    return inputs, actions, revards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjaE68ceE3lc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e07bc4d-e8ca-480b-85ec-8e4cd553242a"
      },
      "source": [
        "for episodes in range(100):\n",
        "  x_other1 = torch.FloatTensor([1] * 3) / 3\n",
        "  x_other1.requires_grad_(True)\n",
        "  opt_other1 = torch.optim.SGD([x_other1], lr=0.1)\n",
        "  x_self1 = [0] * 3\n",
        "  x_self1[environment.agents_goals[0]] = 1\n",
        "  x_self1 = torch.FloatTensor(x_self1)\n",
        "  x_self1.requires_grad_(False)\n",
        "\n",
        "  x_other2 = torch.FloatTensor([1] * 3) / 3\n",
        "  x_other2.requires_grad_(True)\n",
        "  opt_other2 = torch.optim.SGD([x_other2], lr=0.1)\n",
        "  x_self2 = [0] * 3\n",
        "  x_self2[environment.agents_goals[1]] = 1\n",
        "  x_self2 = torch.FloatTensor(x_self2)\n",
        "  x_self2.requires_grad_(False)\n",
        "\n",
        "  environment = env(8, 20)\n",
        "\n",
        "  action1, action2 = 0, 0\n",
        "\n",
        "  seq1 = []\n",
        "  seq2 = []\n",
        "\n",
        "  for step in range(10):\n",
        "    #Make action\n",
        "    state1 = environment.observation(0)\n",
        "    input1 = torch.cat([torch.Tensor(state1), x_self1, x_other1])\n",
        "    probs, _ = agent1.forward(input1)\n",
        "    action1 = torch.argmax(probs)\n",
        "\n",
        "    \n",
        "    state2 = environment.observation(1)\n",
        "    input2 = torch.cat([torch.Tensor(state2), x_self2, x_other2])\n",
        "    probs, _ = agent2.forward(input2)\n",
        "    action2 = torch.argmax(probs)\n",
        "\n",
        "    #Update SOM\n",
        "    opt_other1.zero_grad()\n",
        "    input_other1 = torch.cat([torch.Tensor(state2), x_other1, x_self1])\n",
        "    probs1, _ = agent1.forward(input_other1)\n",
        "    loss = torch.nn.functional.cross_entropy(probs1.view(1, 5), torch.LongTensor([action2.item()]))\n",
        "    loss.backward()\n",
        "    opt_other1.step()\n",
        "\n",
        "\n",
        "    opt_other2.zero_grad()\n",
        "    input_other2 = torch.cat([torch.Tensor(state1), x_other2, x_self2])\n",
        "    probs2, _ = agent2.forward(input_other2)\n",
        "    loss = torch.nn.functional.cross_entropy(probs2.view(1, 5), torch.LongTensor([action1.item()]))\n",
        "    loss.backward()\n",
        "    opt_other2.step()\n",
        "\n",
        "\n",
        "    #Add SA to sequences\n",
        "    environment.step(0, action1.item())\n",
        "    seq1.append([input1.tolist(), action1.tolist()])\n",
        "    \n",
        "    environment.step(1, action2.item())\n",
        "    seq2.append([input2.tolist(), action2.tolist()])\n",
        "  \n",
        "  #Add sequences to buffer\n",
        "  reward = environment.reward()\n",
        "  print(reward)\n",
        "  for i in range(10):\n",
        "    buffer_ag_1.append(seq1[-i - 1] + [reward])\n",
        "    buffer_ag_2.append(seq2[-i - 1] + [reward])\n",
        "    reward *= 0.99\n",
        "\n",
        "  #A2C update\n",
        "  if(len(buffer_ag_1) >= batch_size and len(buffer_ag_2) >= batch_size):\n",
        "    inputs, actions, rewards = sample_from_buffer(buffer_ag_1, batch_size)\n",
        "    opt1.zero_grad()\n",
        "    loss = torch.sum(agent1.get_a2c_loss(actions, inputs, rewards)) / batch_size\n",
        "    print(loss, ' ', end='')\n",
        "    loss.backward()\n",
        "    opt1.step()\n",
        "\n",
        "    inputs, actions, rewards = sample_from_buffer(buffer_ag_2, batch_size)\n",
        "    opt2.zero_grad()\n",
        "    loss = torch.sum(agent2.get_a2c_loss(actions, inputs, rewards)) / batch_size\n",
        "    print(loss, end='\\n')\n",
        "    loss.backward()\n",
        "    opt2.step()\n",
        "\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor(0.1064, grad_fn=<DivBackward0>)  tensor(1.3967, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2989, grad_fn=<DivBackward0>)  tensor(0.3158, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.5265, grad_fn=<DivBackward0>)  tensor(-0.0336, grad_fn=<DivBackward0>)\n",
            "1\n",
            "tensor(0.0138, grad_fn=<DivBackward0>)  tensor(0.1359, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1726, grad_fn=<DivBackward0>)  tensor(0.2256, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1117, grad_fn=<DivBackward0>)  tensor(-0.1034, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0905, grad_fn=<DivBackward0>)  tensor(-0.0787, grad_fn=<DivBackward0>)\n",
            "1\n",
            "tensor(0.4294, grad_fn=<DivBackward0>)  tensor(0.1602, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.9349, grad_fn=<DivBackward0>)  tensor(-0.3310, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(2.3235, grad_fn=<DivBackward0>)  tensor(0.2234, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1839, grad_fn=<DivBackward0>)  tensor(0.3062, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.8811, grad_fn=<DivBackward0>)  tensor(-0.0224, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.3836, grad_fn=<DivBackward0>)  tensor(0.2444, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4462, grad_fn=<DivBackward0>)  tensor(0.2172, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1715, grad_fn=<DivBackward0>)  tensor(0.4634, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.1933, grad_fn=<DivBackward0>)  tensor(0.0806, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.2925, grad_fn=<DivBackward0>)  tensor(0.7804, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.5075, grad_fn=<DivBackward0>)  tensor(0.2202, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4576, grad_fn=<DivBackward0>)  tensor(-0.2838, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2282, grad_fn=<DivBackward0>)  tensor(0.3270, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4277, grad_fn=<DivBackward0>)  tensor(0.1387, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0916, grad_fn=<DivBackward0>)  tensor(0.1864, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0752, grad_fn=<DivBackward0>)  tensor(0.1363, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.1645, grad_fn=<DivBackward0>)  tensor(0.3975, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.9376, grad_fn=<DivBackward0>)  tensor(-0.0157, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.3458, grad_fn=<DivBackward0>)  tensor(-0.2124, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0102, grad_fn=<DivBackward0>)  tensor(-0.0601, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1398, grad_fn=<DivBackward0>)  tensor(0.4899, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.6004, grad_fn=<DivBackward0>)  tensor(-0.5985, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.9560, grad_fn=<DivBackward0>)  tensor(0.6040, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4359, grad_fn=<DivBackward0>)  tensor(0.2367, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2751, grad_fn=<DivBackward0>)  tensor(0.2110, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.6021, grad_fn=<DivBackward0>)  tensor(0.3736, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0058, grad_fn=<DivBackward0>)  tensor(0.3118, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2159, grad_fn=<DivBackward0>)  tensor(0.3825, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0375, grad_fn=<DivBackward0>)  tensor(0.5042, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4720, grad_fn=<DivBackward0>)  tensor(-0.1339, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.3887, grad_fn=<DivBackward0>)  tensor(-0.0648, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.2388, grad_fn=<DivBackward0>)  tensor(-0.1516, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4607, grad_fn=<DivBackward0>)  tensor(-0.1390, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2117, grad_fn=<DivBackward0>)  tensor(-0.1088, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.5565, grad_fn=<DivBackward0>)  tensor(0.0799, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.6152, grad_fn=<DivBackward0>)  tensor(-0.0465, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0358, grad_fn=<DivBackward0>)  tensor(0.0061, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0897, grad_fn=<DivBackward0>)  tensor(-0.1270, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0904, grad_fn=<DivBackward0>)  tensor(0.0345, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0213, grad_fn=<DivBackward0>)  tensor(-0.0447, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0924, grad_fn=<DivBackward0>)  tensor(0.0570, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.6219, grad_fn=<DivBackward0>)  tensor(-0.1713, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.9773, grad_fn=<DivBackward0>)  tensor(0.5170, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1924, grad_fn=<DivBackward0>)  tensor(0.5759, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1833, grad_fn=<DivBackward0>)  tensor(0.3162, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.5902, grad_fn=<DivBackward0>)  tensor(-0.0512, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(1.0932, grad_fn=<DivBackward0>)  tensor(-0.9694, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0363, grad_fn=<DivBackward0>)  tensor(-0.1007, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.7892, grad_fn=<DivBackward0>)  tensor(-0.0744, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.4971, grad_fn=<DivBackward0>)  tensor(-0.1302, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1287, grad_fn=<DivBackward0>)  tensor(0.0241, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1193, grad_fn=<DivBackward0>)  tensor(-0.0692, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1992, grad_fn=<DivBackward0>)  tensor(0.0675, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0455, grad_fn=<DivBackward0>)  tensor(-0.0314, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0655, grad_fn=<DivBackward0>)  tensor(-1.4943, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0916, grad_fn=<DivBackward0>)  tensor(-1.5615, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-1.0506, grad_fn=<DivBackward0>)  tensor(-0.0302, grad_fn=<DivBackward0>)\n",
            "-1\n",
            "tensor(-0.0540, grad_fn=<DivBackward0>)  tensor(0.2710, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.5216, grad_fn=<DivBackward0>)  tensor(0.1608, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.3900, grad_fn=<DivBackward0>)  tensor(0.7019, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2753, grad_fn=<DivBackward0>)  tensor(-0.6269, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2669, grad_fn=<DivBackward0>)  tensor(0.3522, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2756, grad_fn=<DivBackward0>)  tensor(-0.0490, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1290, grad_fn=<DivBackward0>)  tensor(-0.9197, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0180, grad_fn=<DivBackward0>)  tensor(-1.4868, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0143, grad_fn=<DivBackward0>)  tensor(-1.5196, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1219, grad_fn=<DivBackward0>)  tensor(-0.1712, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1647, grad_fn=<DivBackward0>)  tensor(-7.7482, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0017, grad_fn=<DivBackward0>)  tensor(0.8027, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1226, grad_fn=<DivBackward0>)  tensor(2.4888, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.0492, grad_fn=<DivBackward0>)  tensor(0.4058, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1268, grad_fn=<DivBackward0>)  tensor(1.1792, grad_fn=<DivBackward0>)\n",
            "-4\n",
            "tensor(0.0182, grad_fn=<DivBackward0>)  tensor(0.7397, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-1.1077, grad_fn=<DivBackward0>)  tensor(-0.6311, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1990, grad_fn=<DivBackward0>)  tensor(-0.3923, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.3535, grad_fn=<DivBackward0>)  tensor(-2.8300, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.0339, grad_fn=<DivBackward0>)  tensor(-3.2051, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2549, grad_fn=<DivBackward0>)  tensor(-2.9819, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1777, grad_fn=<DivBackward0>)  tensor(-2.0423, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.6153, grad_fn=<DivBackward0>)  tensor(-4.7846, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.3129, grad_fn=<DivBackward0>)  tensor(0.5618, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2412, grad_fn=<DivBackward0>)  tensor(2.7395, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1648, grad_fn=<DivBackward0>)  tensor(2.8796, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1255, grad_fn=<DivBackward0>)  tensor(4.9865, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.2179, grad_fn=<DivBackward0>)  tensor(0.3546, grad_fn=<DivBackward0>)\n",
            "-1\n",
            "tensor(0.2244, grad_fn=<DivBackward0>)  tensor(-5.4072, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.6930, grad_fn=<DivBackward0>)  tensor(-2.6983, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(-0.1139, grad_fn=<DivBackward0>)  tensor(-0.9993, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.2643, grad_fn=<DivBackward0>)  tensor(-3.3108, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.6520, grad_fn=<DivBackward0>)  tensor(-3.5942, grad_fn=<DivBackward0>)\n",
            "1\n",
            "tensor(-0.1162, grad_fn=<DivBackward0>)  tensor(-11.1175, grad_fn=<DivBackward0>)\n",
            "0\n",
            "tensor(0.1078, grad_fn=<DivBackward0>)  tensor(-4.9551, grad_fn=<DivBackward0>)\n",
            "1\n",
            "tensor(-0.0949, grad_fn=<DivBackward0>)  tensor(-15.4214, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VSHZDCT_sqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49187efe-8ec4-4178-c1f0-41c17f8984d9"
      },
      "source": [
        "x = torch.FloatTensor([0.5, 0.3, 0.2])\n",
        "y = torch.FloatTensor([0.5, 0.3, 0.2])\n",
        "x.tolist()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5, 0.30000001192092896, 0.20000000298023224]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    }
  ]
}