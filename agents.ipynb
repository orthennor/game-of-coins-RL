{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/dexfrost89/coin+game\" target=\"_blank\">https://app.wandb.ai/dexfrost89/coin+game</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/dexfrost89/coin+game/runs/hsfm3y2i\" target=\"_blank\">https://app.wandb.ai/dexfrost89/coin+game/runs/hsfm3y2i</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:181: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04942328855395317  -0.16911320388317108\n",
      "0.0\n",
      "0.04358840733766556  0.2628720998764038\n",
      "0.0\n",
      "1.9291702508926392  1.8190314769744873\n",
      "1.0\n",
      "30.68630027770996  31.078454971313477\n",
      "7.0\n",
      "2.0655932426452637  1.5969560146331787\n",
      "1.0\n",
      "1.5292800664901733  1.69171142578125\n",
      "1.0\n",
      "3.856539249420166  3.6289143562316895\n",
      "2.0\n",
      "-0.1896585375070572  -0.349037230014801\n",
      "0.0\n",
      "-0.10577650368213654  -0.29012686014175415\n",
      "0.0\n",
      "-0.45210400223731995  -0.4686129093170166\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(entity = \"dexfrost89\", project=\"coin game\", name=\"10\")\n",
    "\n",
    "session_saver = []\n",
    "\n",
    "class env:\n",
    "    def __init__(self, size, step_limit, debug=0):\n",
    "        self.number_of_resources = 3\n",
    "        self.amount_of_resource = 4\n",
    "        self.debug = debug\n",
    "        self.size = size\n",
    "        self.step_limit = step_limit\n",
    "        #self.valid_moves = ['up', 'down', 'left', 'right', 'pass']\n",
    "        self.valid_moves = [0, 1, 2, 3, 4]\n",
    "        self.move_map = {0:[-1,0], \n",
    "                         1:[1,0], \n",
    "                         2:[0,-1], \n",
    "                         3:[0,1], \n",
    "                         4:[0,0]}\n",
    "        \n",
    "        self.initialize_map()\n",
    "        self.initialize_resources()\n",
    "        \n",
    "\n",
    "    \n",
    "    def initialize_map(self):\n",
    "        self.map = np.zeros((self.size, self.size), dtype=int)\n",
    "        agent_rand_loc = random.randint(0,1)\n",
    "        if agent_rand_loc:\n",
    "            agent_1_coords = [0,self.size -1]\n",
    "            self.map[0,self.size -1 ] = -1\n",
    "            agent_2_coords = [0,0]\n",
    "            self.map[0, 0] = -2 \n",
    "            \n",
    "        else:\n",
    "            agent_1_coords = [0,0]\n",
    "            self.map[0,0] = -1\n",
    "            agent_2_coords = [0,self.size-1]\n",
    "            self.map[0,self.size-1] = -2\n",
    "        self.agents_coords = [agent_1_coords, agent_2_coords]\n",
    "        \n",
    "    def initialize_resources(self):\n",
    "        self.resources = []\n",
    "        index = [i for i in range(self.size**2) if (i != 0) and (i != self.size-1)]\n",
    "        all_indexes = random.sample(index, self.number_of_resources*self.amount_of_resource)\n",
    "        res_ind = []\n",
    "        for i in range(self.number_of_resources):\n",
    "            resource = random.sample(all_indexes, self.amount_of_resource)\n",
    "            all_indexes = [r for r in all_indexes if r not in resource]\n",
    "            resource = [(int(r/self.size),r%self.size) for r in resource]\n",
    "            empty_map = np.zeros((self.size, self.size), dtype=int)\n",
    "            for ind in range(len(resource)):\n",
    "                x, y = resource[ind][0], resource[ind][1]                \n",
    "                self.map[x, y] = i+1\n",
    "                empty_map[x, y] = 1\n",
    "            self.resources.append(empty_map)\n",
    "        goal_index = [i for i in range(self.number_of_resources)]\n",
    "        goals = random.sample(goal_index, 2)\n",
    "        self.agents_goals = goals\n",
    "        self.agents_res_collected = [[], []]\n",
    "        self.agents_self_goal_collected = [0, 0]\n",
    "        self.agents_othr_goal_collected = [0, 0]\n",
    "        self.agents_nthr_goal_collected = [0, 0]\n",
    "                \n",
    "    def step(self, agent_numb, move_code):\n",
    "        if self.step_limit == 0:\n",
    "            if self.debug:\n",
    "                print('step limit reached, error')\n",
    "        else:\n",
    "            self.step_limit -= 1\n",
    "            if self.debug:\n",
    "                print(self.step_limit, 'steps left')\n",
    "        if move_code not in self.valid_moves:\n",
    "            if self.debug:\n",
    "                print('icorrect move')\n",
    "        else:\n",
    "            agent_old_coords = self.agents_coords[agent_numb]\n",
    "            move = self.move_map[move_code]\n",
    "            agent_new_coords = l3 = [l+r for l,r in zip(agent_old_coords, move)]\n",
    "            correct_move = True\n",
    "            for coord in agent_new_coords:\n",
    "                if (coord < 0) or (coord >= self.size):\n",
    "                    if self.debug:\n",
    "                        print('icorrect move')\n",
    "                    correct_move = False\n",
    "            other_coords = self.agents_coords[1-agent_numb]\n",
    "            if agent_new_coords == other_coords:\n",
    "                if self.debug:\n",
    "                    print('icorrect move')\n",
    "                correct_move = False\n",
    "            if correct_move:\n",
    "                self.map[agent_old_coords[0],agent_old_coords[1]] = 0\n",
    "                self.map[agent_new_coords[0],agent_new_coords[1]] = -1 - agent_numb\n",
    "                self.agents_coords[agent_numb] = agent_new_coords\n",
    "                for res in range(self.number_of_resources):\n",
    "                    if self.resources[res][agent_new_coords[0],agent_new_coords[1]] != 0:\n",
    "                        self.agents_res_collected[agent_numb].append(res)\n",
    "                        self.resources[res][agent_new_coords[0],agent_new_coords[1]] = 0\n",
    "                        if res == self.agents_goals[agent_numb]:\n",
    "                            self.agents_self_goal_collected[agent_numb]+=1\n",
    "                            if self.debug:\n",
    "                                print('collected self goal resource:', res)\n",
    "                        elif res == self.agents_goals[1 - agent_numb]:\n",
    "                            self.agents_othr_goal_collected[agent_numb]+=1\n",
    "                            if self.debug:\n",
    "                                print('collected other goal resource:', res)\n",
    "                        else:\n",
    "                            self.agents_nthr_goal_collected[agent_numb]+=1\n",
    "                            if self.debug:\n",
    "                                print('collected neither goal resource:', res)\n",
    "                            \n",
    "    def observation(self, agent_numb):\n",
    "        feature_vec = []\n",
    "        self_other_coords = [self.agents_coords[agent_numb], \n",
    "                             self.agents_coords[1 - agent_numb] \n",
    "                            ]\n",
    "        for agnt_coords in self_other_coords:\n",
    "            agent_map = np.zeros((self.size, self.size), dtype=int)\n",
    "            agent_map[agnt_coords[0],agnt_coords[1]] = 1\n",
    "            agent_map = np.reshape(agent_map, self.size*self.size)\n",
    "            feature_vec.extend(agent_map)\n",
    "            \n",
    "        for res in range(len(self.resources)):\n",
    "            feature_vec.extend(np.reshape(self.resources[res], self.size*self.size))\n",
    "        \n",
    "        return feature_vec\n",
    "    \n",
    "    def reward(self):\n",
    "        \n",
    "        n_self_c_self = self.agents_self_goal_collected[0]\n",
    "        n_othr_c_self = self.agents_othr_goal_collected[1]\n",
    "        \n",
    "        n_self_c_othr = self.agents_othr_goal_collected[0]\n",
    "        n_othr_c_othr = self.agents_self_goal_collected[1]\n",
    "        \n",
    "        n_self_c_nthr = self.agents_nthr_goal_collected[0]\n",
    "        n_othr_c_nthr = self.agents_nthr_goal_collected[1]\n",
    "        \n",
    "        reward = (n_self_c_self + n_othr_c_self)**2 + \\\n",
    "                 (n_self_c_othr + n_othr_c_othr)**2 - \\\n",
    "                 (n_self_c_nthr + n_othr_c_nthr)**2 \n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_limit = step_limit\n",
    "        self.initialize_map()\n",
    "        self.initialize_resources()\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "class Ffunction(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Ffunction, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, 64)\n",
    "        self.fullycon1 = nn.Linear(64, 64)\n",
    "        nn.init.orthogonal_(self.fullycon1.weight)\n",
    "        self.fullycon2 = nn.Linear(64, 64)\n",
    "        nn.init.orthogonal_(self.fullycon2.weight)\n",
    "        self.policy = nn.Linear(64, 5)\n",
    "        nn.init.orthogonal_(self.policy.weight)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "        nn.init.orthogonal_(self.value.weight)\n",
    "\n",
    "        self.beta = 0.01\n",
    "\n",
    "    def forward(self, x): #x = input\n",
    "        result, _ = self.lstm(x.view(-1, 1, self.input_size))\n",
    "        result = nn.functional.elu(self.fullycon1(result.view(-1, 64)))\n",
    "        result = nn.functional.elu(self.fullycon2(result))\n",
    "        return nn.functional.softmax(self.policy(result)), self.value(result)\n",
    "\n",
    "    def get_a2c_loss(self, states, actions, rewards):\n",
    "        probs, values = self.forward(states)\n",
    "\n",
    "        #print(\"rewards/values\")\n",
    "        #print(rewards)\n",
    "        #print()\n",
    "        #print(values)\n",
    "\n",
    "        #print(torch.argmax(probs, dim=1))\n",
    "        #print(actions)\n",
    "\n",
    "        advantage = rewards - values.view(batch_size)\n",
    "        actions_one_hot = torch.zeros(batch_size, 5)\n",
    "        actions_one_hot[torch.arange(batch_size), actions.view(batch_size)] = 1\n",
    "        logs = torch.log(torch.sum(probs.view(batch_size, 5) * actions_one_hot.view(batch_size, 5), (1)))\n",
    "        #print(values)\n",
    "        logloss = -logs * (advantage.view(batch_size)).detach()\n",
    "        entropyloss = -torch.sum(probs * torch.log(probs), (1))\n",
    "        #print(\"\\nlogs\", logs)\n",
    "\n",
    "        return logloss + self.beta * entropyloss + 0.5 * advantage * advantage\n",
    "\n",
    "agent1 = Ffunction(326)\n",
    "opt1 = torch.optim.Adam(agent1.parameters(), betas=(0.9, 0.999), eps=1 * 10 ** -8)\n",
    "\n",
    "agent2 = Ffunction(326)\n",
    "opt2 = torch.optim.Adam(agent2.parameters(), betas=(0.9, 0.999), eps=1 * 10 ** -8)\n",
    "\n",
    "# buffer_ag_N = [[input, action, revard]*number_of_samples]\n",
    "buffer_ag_1 = []\n",
    "buffer_ag_2 = []\n",
    "reward_buf = []\n",
    "\n",
    "def sample_from_buffer(buffer, batch_size=10):\n",
    "    batch = random.sample(buffer, batch_size)\n",
    "    inputs = [i[0] for i in batch]\n",
    "    actions = [i[1] for i in batch]\n",
    "    revards = [i[2] for i in batch]\n",
    "    \n",
    "    inputs = torch.FloatTensor(inputs)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    revards = torch.FloatTensor(revards)\n",
    "    \n",
    "    return inputs, actions, revards\n",
    "\n",
    "def sample_buffer(buffer):\n",
    "    inputs = [i[0] for i in buffer]\n",
    "    actions = [i[1] for i in buffer]\n",
    "    revards = [i[2] for i in buffer]\n",
    "\n",
    "    inputs = torch.FloatTensor(inputs)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    revards = torch.FloatTensor(revards)\n",
    "\n",
    "    return inputs, actions, revards\n",
    "\n",
    "for episode in range(1, 11):\n",
    "\n",
    "  \n",
    "    for _ in range(batch_size // 10):\n",
    "        session_saver.append([])\n",
    "        environment = env(8, 20)\n",
    "        session_saver[-1].append(environment.map.tolist())\n",
    "\n",
    "        x_other1 = torch.FloatTensor([1] * 3) / 3\n",
    "        x_other1.requires_grad_(True)\n",
    "        opt_other1 = torch.optim.SGD([x_other1], lr=0.1)\n",
    "        x_self1 = [0] * 3\n",
    "        x_self1[environment.agents_goals[0]] = 1\n",
    "        x_self1 = torch.FloatTensor(x_self1)\n",
    "        x_self1.requires_grad_(False)\n",
    "\n",
    "        x_other2 = torch.FloatTensor([1] * 3) / 3\n",
    "        x_other2.requires_grad_(True)\n",
    "        opt_other2 = torch.optim.SGD([x_other2], lr=0.1)\n",
    "        x_self2 = [0] * 3\n",
    "        x_self2[environment.agents_goals[1]] = 1\n",
    "        x_self2 = torch.FloatTensor(x_self2)\n",
    "        x_self2.requires_grad_(False)\n",
    "\n",
    "\n",
    "        action1, action2 = 0, 0\n",
    "\n",
    "        seq1 = []\n",
    "        seq2 = []\n",
    "\n",
    "        for step in range(10):\n",
    "            #Make action\n",
    "            state1 = environment.observation(0)\n",
    "            input1 = torch.cat([torch.Tensor(state1), x_self1, x_other1])\n",
    "            probs1, values1 = agent1.forward(input1)\n",
    "            action1 = torch.argmax(probs1)\n",
    "            seq1.append([input1.tolist(), action1.tolist()])\n",
    "\n",
    "\n",
    "            state2 = environment.observation(1)\n",
    "            input2 = torch.cat([torch.Tensor(state2), x_self2, x_other2])\n",
    "            probs2, values2 = agent2.forward(input2)\n",
    "            action2 = torch.argmax(probs2)\n",
    "            seq2.append([input2.tolist(), action2.tolist()])\n",
    "\n",
    "            #Update SOM\n",
    "            opt_other1.zero_grad()\n",
    "            input_other1 = torch.cat([torch.Tensor(state2), x_other1, x_self1])\n",
    "            probs1, _ = agent1.forward(input_other1)\n",
    "            loss = torch.nn.functional.cross_entropy(probs1.view(1, 5), torch.LongTensor([action2.item()]))\n",
    "            loss.backward()\n",
    "            opt_other1.step()\n",
    "\n",
    "            probs1, values1 = agent1.forward(input1)\n",
    "            #print(action1.item() == torch.argmax(probs1).item())\n",
    "\n",
    "            opt_other2.zero_grad()\n",
    "            input_other2 = torch.cat([torch.Tensor(state1), x_other2, x_self2])\n",
    "            probs2, _ = agent2.forward(input_other2)\n",
    "            loss = torch.nn.functional.cross_entropy(probs2.view(1, 5), torch.LongTensor([action1.item()]))\n",
    "            loss.backward()\n",
    "            opt_other2.step()\n",
    "\n",
    "\n",
    "            #Add SA to sequences\n",
    "            environment.step(0, action1.item())\n",
    "            session_saver[-1].append(environment.map.tolist())\n",
    "\n",
    "            environment.step(1, action2.item())\n",
    "            session_saver[-1].append(environment.map.tolist())\n",
    "    \n",
    "    #Add sequences to buffer\n",
    "        reward = environment.reward()\n",
    "        reward_buf.append(reward)\n",
    "        for i in range(10):\n",
    "            buffer_ag_1.append(seq1[-i - 1] + [reward])\n",
    "            buffer_ag_2.append(seq2[-i - 1] + [reward])\n",
    "            reward *= 0.99\n",
    "\n",
    "    #A2C update\n",
    "    if(len(buffer_ag_1) >= batch_size and len(buffer_ag_2) >= batch_size):\n",
    "        inputs, actions, rewards = sample_buffer(buffer_ag_1)\n",
    "        opt1.zero_grad()\n",
    "        loss1 = torch.sum(agent1.get_a2c_loss(inputs, actions, rewards)) / batch_size\n",
    "        print(loss1.item(), ' ', end='')\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        inputs, actions, rewards = sample_buffer(buffer_ag_2)\n",
    "        opt2.zero_grad()\n",
    "        loss2 = torch.sum(agent2.get_a2c_loss(inputs, actions, rewards)) / batch_size\n",
    "        print(loss2.item(), end='\\n')\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        print(np.mean(reward_buf))\n",
    "\n",
    "        wandb.log({\"loss1\": loss1.item(), \"loss2\": loss2.item(), \"reward\": np.mean(reward_buf), \"episode\": episode})\n",
    "\n",
    "        buffer_ag_1 = []\n",
    "        buffer_ag_2 = []\n",
    "        reward_buf = []\n",
    "\n",
    "import pickle\n",
    "\n",
    "ftw = open('save10', 'wb')\n",
    "\n",
    "pickle.dump(session_saver, file=ftw)\n",
    "ftw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
