{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1efU_2zZhEZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e7065a9f-1e5c-4909-949f-2121201a8685"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "cuda = torch.device('cpu')\n",
        "\n",
        "\n",
        "wandb.init(entity = \"dexfrost89\", project=\"coin game\", name=str(batch_size))\n",
        "\n",
        "session_saver = []\n",
        "\n",
        "class env:\n",
        "    def __init__(self, size, step_limit, debug=0):\n",
        "        self.number_of_resources = 3\n",
        "        self.amount_of_resource = 4\n",
        "        self.debug = debug\n",
        "        self.size = size\n",
        "        self.step_limit = step_limit\n",
        "        #self.valid_moves = ['up', 'down', 'left', 'right', 'pass']\n",
        "        self.valid_moves = [0, 1, 2, 3, 4]\n",
        "        self.move_map = {0:[-1,0], \n",
        "                         1:[1,0], \n",
        "                         2:[0,-1], \n",
        "                         3:[0,1], \n",
        "                         4:[0,0]}\n",
        "        \n",
        "        self.initialize_map()\n",
        "        self.initialize_resources()\n",
        "        \n",
        "\n",
        "    \n",
        "    def initialize_map(self):\n",
        "        self.map = np.zeros((self.size, self.size), dtype=int)\n",
        "        agent_rand_loc = random.randint(0,1)\n",
        "        if agent_rand_loc:\n",
        "            agent_1_coords = [0,self.size -1]\n",
        "            self.map[0,self.size -1 ] = -1\n",
        "            agent_2_coords = [0,0]\n",
        "            self.map[0, 0] = -2 \n",
        "            \n",
        "        else:\n",
        "            agent_1_coords = [0,0]\n",
        "            self.map[0,0] = -1\n",
        "            agent_2_coords = [0,self.size-1]\n",
        "            self.map[0,self.size-1] = -2\n",
        "        self.agents_coords = [agent_1_coords, agent_2_coords]\n",
        "        \n",
        "    def initialize_resources(self):\n",
        "        self.resources = []\n",
        "        index = [i for i in range(self.size**2) if (i != 0) and (i != self.size-1)]\n",
        "        all_indexes = random.sample(index, self.number_of_resources*self.amount_of_resource)\n",
        "        res_ind = []\n",
        "        for i in range(self.number_of_resources):\n",
        "            resource = random.sample(all_indexes, self.amount_of_resource)\n",
        "            all_indexes = [r for r in all_indexes if r not in resource]\n",
        "            resource = [(int(r/self.size),r%self.size) for r in resource]\n",
        "            empty_map = np.zeros((self.size, self.size), dtype=int)\n",
        "            for ind in range(len(resource)):\n",
        "                x, y = resource[ind][0], resource[ind][1]                \n",
        "                self.map[x, y] = i+1\n",
        "                empty_map[x, y] = 1\n",
        "            self.resources.append(empty_map)\n",
        "        goal_index = [i for i in range(self.number_of_resources)]\n",
        "        goals = random.sample(goal_index, 2)\n",
        "        self.agents_goals = goals\n",
        "        self.agents_res_collected = [[], []]\n",
        "        self.agents_self_goal_collected = [0, 0]\n",
        "        self.agents_othr_goal_collected = [0, 0]\n",
        "        self.agents_nthr_goal_collected = [0, 0]\n",
        "                \n",
        "    def step(self, agent_numb, move_code):\n",
        "        if self.step_limit == 0:\n",
        "            if self.debug:\n",
        "                print('step limit reached, error')\n",
        "        else:\n",
        "            self.step_limit -= 1\n",
        "            if self.debug:\n",
        "                print(self.step_limit, 'steps left')\n",
        "        if move_code not in self.valid_moves:\n",
        "            if self.debug:\n",
        "                print('icorrect move')\n",
        "        else:\n",
        "            agent_old_coords = self.agents_coords[agent_numb]\n",
        "            move = self.move_map[move_code]\n",
        "            agent_new_coords = l3 = [l+r for l,r in zip(agent_old_coords, move)]\n",
        "            correct_move = True\n",
        "            for coord in agent_new_coords:\n",
        "                if (coord < 0) or (coord >= self.size):\n",
        "                    if self.debug:\n",
        "                        print('icorrect move')\n",
        "                    correct_move = False\n",
        "            other_coords = self.agents_coords[1-agent_numb]\n",
        "            if agent_new_coords == other_coords:\n",
        "                if self.debug:\n",
        "                    print('icorrect move')\n",
        "                correct_move = False\n",
        "            if correct_move:\n",
        "                self.map[agent_old_coords[0],agent_old_coords[1]] = 0\n",
        "                self.map[agent_new_coords[0],agent_new_coords[1]] = -1 - agent_numb\n",
        "                self.agents_coords[agent_numb] = agent_new_coords\n",
        "                for res in range(self.number_of_resources):\n",
        "                    if self.resources[res][agent_new_coords[0],agent_new_coords[1]] != 0:\n",
        "                        self.agents_res_collected[agent_numb].append(res)\n",
        "                        self.resources[res][agent_new_coords[0],agent_new_coords[1]] = 0\n",
        "                        if res == self.agents_goals[agent_numb]:\n",
        "                            self.agents_self_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected self goal resource:', res)\n",
        "                        elif res == self.agents_goals[1 - agent_numb]:\n",
        "                            self.agents_othr_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected other goal resource:', res)\n",
        "                        else:\n",
        "                            self.agents_nthr_goal_collected[agent_numb]+=1\n",
        "                            if self.debug:\n",
        "                                print('collected neither goal resource:', res)\n",
        "                            \n",
        "    def observation(self, agent_numb):\n",
        "        feature_vec = []\n",
        "        self_other_coords = [self.agents_coords[agent_numb], \n",
        "                             self.agents_coords[1 - agent_numb] \n",
        "                            ]\n",
        "        for agnt_coords in self_other_coords:\n",
        "            agent_map = np.zeros((self.size, self.size), dtype=int)\n",
        "            agent_map[agnt_coords[0],agnt_coords[1]] = 1\n",
        "            agent_map = np.reshape(agent_map, self.size*self.size)\n",
        "            feature_vec.extend(agent_map)\n",
        "            \n",
        "        for res in range(len(self.resources)):\n",
        "            feature_vec.extend(np.reshape(self.resources[res], self.size*self.size))\n",
        "        \n",
        "        return feature_vec\n",
        "    \n",
        "    def reward(self):\n",
        "        \n",
        "        n_self_c_self = self.agents_self_goal_collected[0]\n",
        "        n_othr_c_self = self.agents_othr_goal_collected[1]\n",
        "        \n",
        "        n_self_c_othr = self.agents_othr_goal_collected[0]\n",
        "        n_othr_c_othr = self.agents_self_goal_collected[1]\n",
        "        \n",
        "        n_self_c_nthr = self.agents_nthr_goal_collected[0]\n",
        "        n_othr_c_nthr = self.agents_nthr_goal_collected[1]\n",
        "        \n",
        "        reward = (n_self_c_self + n_othr_c_self)**2 + \\\n",
        "                 (n_self_c_othr + n_othr_c_othr)**2 - \\\n",
        "                 (n_self_c_nthr + n_othr_c_nthr)**2 \n",
        "        \n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_limit = step_limit\n",
        "        self.initialize_map()\n",
        "        self.initialize_resources()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://app.wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/dexfrost89/coin%20game\" target=\"_blank\">https://app.wandb.ai/dexfrost89/coin%20game</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/dexfrost89/coin%20game/runs/3hcq14ew\" target=\"_blank\">https://app.wandb.ai/dexfrost89/coin%20game/runs/3hcq14ew</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RD3hP9iijC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZb9SsD6is2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ffunction(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Ffunction, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.fullycon1 = nn.Linear(self.input_size, 64)\n",
        "        nn.init.orthogonal_(self.fullycon1.weight)\n",
        "        self.fullycon1.bias.data.fill_(0)\n",
        "\n",
        "        self.fullycon2 = nn.Linear(64, 64)\n",
        "        nn.init.orthogonal_(self.fullycon2.weight)\n",
        "        self.fullycon2.bias.data.fill_(0)\n",
        "        \n",
        "        self.lstm = nn.LSTM(64, 64)\n",
        "        nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
        "        nn.init.orthogonal_(self.lstm.weight_ih_l0)\n",
        "        self.lstm.bias_ih_l0.data.fill_(0)\n",
        "        self.lstm.bias_hh_l0.data.fill_(0)\n",
        "\n",
        "        self.policy = nn.Linear(64, 5)\n",
        "        nn.init.orthogonal_(self.policy.weight)\n",
        "        self.policy.bias.data.fill_(0)\n",
        "        self.value = nn.Linear(64, 1)\n",
        "        nn.init.orthogonal_(self.value.weight)\n",
        "        self.value.bias.data.fill_(0)\n",
        "\n",
        "        self.entropy_loss_coef = 0.01\n",
        "        self.value_loss_coef = 0.5\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs, (hx, cx) = inputs\n",
        "        \n",
        "        x = nn.functional.elu(self.fullycon1(inputs))\n",
        "        x = nn.functional.elu(self.fullycon2(x))\n",
        "\n",
        "        x = x.view(-1, 1, 64)\n",
        "\n",
        "        x, (hx, cx) = self.lstm(x, (hx, cx))\n",
        "        x = x.view(-1, 64)\n",
        "        return nn.functional.softmax(self.policy(x)), self.value(x), (hx, cx)\n",
        "\n",
        "    def get_a2c_loss(self, probs, values, rewards):\n",
        "\n",
        "        advantages = rewards - values\n",
        "        value_loss = (advantages.pow(2)).mean()\n",
        "\n",
        "        entropy_loss = (-torch.sum(probs * torch.log(probs), (1))).mean()\n",
        "        \n",
        "        logprobs, _ = torch.max(torch.log(probs), (1))\n",
        "        policy_loss = -(logprobs * advantages.detach()).mean()\n",
        "\n",
        "        #print(policy_loss.item(), entropy_loss.item(), value_loss.item())\n",
        "\n",
        "        return policy_loss - self.entropy_loss_coef * entropy_loss + self.value_loss_coef * value_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV0eOESriyvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ef804d3-d2f3-40b1-a960-a51e3f035ff3"
      },
      "source": [
        "agent1 = Ffunction(326)\n",
        "agent1.to(device=cuda)\n",
        "opt1 = torch.optim.Adam(agent1.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1 * 10 ** -8)\n",
        "\n",
        "agent2 = Ffunction(326)\n",
        "agent2.to(device=cuda)\n",
        "opt2 = torch.optim.Adam(agent2.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1 * 10 ** -8)\n",
        "\n",
        "# buffer_ag_N = [[input, action, revard]*number_of_samples]\n",
        "buffer_ag_1 = []\n",
        "buffer_ag_2 = []\n",
        "reward_buf = []\n",
        "self_coins_collected1 = []\n",
        "self_coins_collected2 = []\n",
        "other_coins_collected1 = []\n",
        "other_coins_collected2 = []\n",
        "neither_coins_collected1 = []\n",
        "neither_coins_collected2 = []\n",
        "\n",
        "def sample_from_buffer(buffer, batch_size=10):\n",
        "    batch = random.sample(buffer, batch_size)\n",
        "    inputs = [i[0] for i in batch]\n",
        "    actions = [i[1] for i in batch]\n",
        "    revards = [i[2] for i in batch]\n",
        "    \n",
        "    inputs = torch.FloatTensor(inputs)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    revards = torch.FloatTensor(revards)\n",
        "    \n",
        "    return inputs, actions, revards\n",
        "\n",
        "def sample_buffer(buffer):\n",
        "    probs = [i[0] for i in buffer]\n",
        "    values = [i[1] for i in buffer]\n",
        "    rewards = [i[2] for i in buffer]\n",
        "\n",
        "    probs = torch.cat(probs)\n",
        "    values = torch.LongTensor(values)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "\n",
        "    return probs, values, rewards\n",
        "\n",
        "for episode in range(1, 100001):\n",
        "\n",
        "    hx11, cx11 = torch.zeros(1, 1, 64), torch.zeros(1, 1, 64)\n",
        "\n",
        "    hx21, cx21 = torch.zeros(1, 1, 64), torch.zeros(1, 1, 64)\n",
        "  \n",
        "    for _ in range(batch_size // 10):\n",
        "        hx1 = hx11.detach()\n",
        "        cx1 = cx11.detach()\n",
        "\n",
        "        hx2 = hx21.detach()\n",
        "        cx2 = cx21.detach()\n",
        "\n",
        "        session_saver.append([])\n",
        "        environment = env(8, 20)\n",
        "        session_saver[-1].append(environment.map.tolist())\n",
        "\n",
        "        x_other1 = torch.FloatTensor([1] * 3).to(device=cuda) / 3\n",
        "        x_other1.requires_grad_(True)\n",
        "        opt_other1 = torch.optim.SGD([x_other1], lr=0.1)\n",
        "        x_self1 = [0] * 3\n",
        "        x_self1[environment.agents_goals[0]] = 1\n",
        "        x_self1 = torch.FloatTensor(x_self1).to(device=cuda)\n",
        "        x_self1.requires_grad_(False)\n",
        "\n",
        "        x_other2 = torch.FloatTensor([1] * 3).to(device=cuda) / 3\n",
        "        x_other2.requires_grad_(True)\n",
        "        opt_other2 = torch.optim.SGD([x_other2], lr=0.1)\n",
        "        x_self2 = [0] * 3\n",
        "        x_self2[environment.agents_goals[1]] = 1\n",
        "        x_self2 = torch.FloatTensor(x_self2).to(device=cuda)\n",
        "        x_self2.requires_grad_(False)\n",
        "\n",
        "\n",
        "        action1, action2 = 0, 0\n",
        "\n",
        "        seq1 = []\n",
        "        seq2 = []\n",
        "\n",
        "        for step in range(10):\n",
        "            #Make action\n",
        "            state1 = environment.observation(0)\n",
        "            input1 = torch.cat([torch.Tensor(state1).to(device=cuda), x_self1, x_other1]).to(device=cuda)\n",
        "            probs1, values1, (hx11, cx11) = agent1.forward((input1, (hx1, cx1)))\n",
        "            action1 = torch.argmax(probs1)\n",
        "            seq1.append([probs1, values1])\n",
        "\n",
        "\n",
        "            state2 = environment.observation(1)\n",
        "            input2 = torch.cat([torch.Tensor(state2).to(device=cuda), x_self2, x_other2]).to(device=cuda)\n",
        "            probs2, values2, (hx21, cx21) = agent2.forward((input2, (hx2, cx2)))\n",
        "            action2 = torch.argmax(probs2)\n",
        "            seq2.append([probs2, values2])\n",
        "\n",
        "            #Update SOM\n",
        "            opt_other1.zero_grad()\n",
        "            input_other1 = torch.cat([torch.Tensor(state2).to(device=cuda), x_other1, x_self1]).to(device=cuda)\n",
        "            probs1, _, _ = agent1.forward((input_other1, (hx2, cx2)))\n",
        "            loss = torch.nn.functional.cross_entropy(probs1.view(1, 5), torch.LongTensor([action2.item()]).to(device=cuda))\n",
        "            loss.backward()\n",
        "            opt_other1.step()\n",
        "\n",
        "            #print(action1.item() == torch.argmax(probs1).item())\n",
        "\n",
        "            opt_other2.zero_grad()\n",
        "            input_other2 = torch.cat([torch.Tensor(state1).to(device=cuda), x_other2, x_self2]).to(device=cuda)\n",
        "            probs2, _, _ = agent2.forward((input_other2, (hx1, cx1)))\n",
        "            loss = torch.nn.functional.cross_entropy(probs2.view(1, 5), torch.LongTensor([action1.item()]).to(device=cuda))\n",
        "            loss.backward()\n",
        "            opt_other2.step()\n",
        "\n",
        "\n",
        "            #Add SA to sequences\n",
        "            environment.step(0, action1.item())\n",
        "            session_saver[-1].append(environment.map.tolist())\n",
        "\n",
        "            environment.step(1, action2.item())\n",
        "            session_saver[-1].append(environment.map.tolist())\n",
        "    \n",
        "    #Add sequences to buffer\n",
        "        reward = environment.reward()\n",
        "        reward_buf.append(reward)\n",
        "        self_coins_collected1.append(environment.agents_self_goal_collected[0])\n",
        "        self_coins_collected2.append(environment.agents_self_goal_collected[1])\n",
        "        other_coins_collected1.append(environment.agents_othr_goal_collected[0])\n",
        "        other_coins_collected2.append(environment.agents_othr_goal_collected[1])\n",
        "        neither_coins_collected1.append(environment.agents_nthr_goal_collected[0])\n",
        "        neither_coins_collected2.append(environment.agents_nthr_goal_collected[1])\n",
        "\n",
        "        for i in range(10):\n",
        "            buffer_ag_1.append(seq1[-i - 1] + [torch.FloatTensor([[reward]])])\n",
        "            buffer_ag_2.append(seq2[-i - 1] + [torch.FloatTensor([[reward]])])\n",
        "            reward *= 0.99\n",
        "\n",
        "    #A2C update\n",
        "    if(len(buffer_ag_1) >= batch_size and len(buffer_ag_2) >= batch_size):\n",
        "\n",
        "        print(np.mean(reward_buf))\n",
        "\n",
        "        inputs, actions, rewards = sample_buffer(buffer_ag_1)\n",
        "        opt1.zero_grad()\n",
        "        loss1 = torch.sum(agent1.get_a2c_loss(inputs.to(device=cuda), actions.to(device=cuda), rewards.to(device=cuda))) / batch_size\n",
        "        print(loss1.item(), ' ', end='')\n",
        "        loss1.backward()\n",
        "        opt1.step()\n",
        "\n",
        "        inputs, actions, rewards = sample_buffer(buffer_ag_2)\n",
        "        opt2.zero_grad()\n",
        "        loss2 = torch.sum(agent2.get_a2c_loss(inputs.to(device=cuda), actions.to(device=cuda), rewards.to(device=cuda))) / batch_size\n",
        "        print(loss2.item(), end='\\n')\n",
        "        loss2.backward()\n",
        "        opt2.step()\n",
        "\n",
        "\n",
        "        wandb.log({\"loss1\": loss1.item(), \"loss2\": loss2.item(), \"reward\": np.mean(reward_buf), \"episode\": episode, \"games\": episode * batch_size // 10, \\\n",
        "                   \"self_coins1\": np.mean(self_coins_collected1), \"self_coins2\": np.mean(self_coins_collected2), \"other_coins1\": np.mean(other_coins_collected1), \\\n",
        "                   \"other_coins2\": np.mean(other_coins_collected2), \"neither_coins1\": np.mean(neither_coins_collected1), \"neither_coins2\": np.mean(neither_coins_collected2)})\n",
        "\n",
        "        buffer_ag_1 = []\n",
        "        buffer_ag_2 = []\n",
        "        reward_buf = []\n",
        "        self_coins_collected1 = []\n",
        "        self_coins_collected2 = []\n",
        "        other_coins_collected1 = []\n",
        "        other_coins_collected2 = []\n",
        "        neither_coins_collected1 = []\n",
        "        neither_coins_collected2 = []\n",
        "\n",
        "import pickle\n",
        "\n",
        "ftw = open('save' + str(batch_size), 'wb')\n",
        "\n",
        "pickle.dump(session_saver, file=ftw)\n",
        "ftw.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.55\n",
            "0.0026675343979150057  0.002664212603121996\n",
            "0.42\n",
            "0.002639257814735174  0.002640261547639966\n",
            "0.99\n",
            "0.004864643793553114  0.0048658642917871475\n",
            "0.89\n",
            "0.007156666833907366  0.007151665631681681\n",
            "0.7\n",
            "0.0034937458112835884  0.0035018501803278923\n",
            "0.87\n",
            "0.00555397430434823  0.00554636400192976\n",
            "0.73\n",
            "0.003878618124872446  0.0038714371621608734\n",
            "1.2\n",
            "0.0053109838627278805  0.005296173505485058\n",
            "1.23\n",
            "0.00501578813418746  0.005000838078558445\n",
            "0.55\n",
            "0.002908026799559593  0.0028940278571099043\n",
            "1.04\n",
            "0.005117109511047602  0.0050837756134569645\n",
            "1.02\n",
            "0.005049831699579954  0.005033153109252453\n",
            "1.03\n",
            "0.004983439575880766  0.00495892483741045\n",
            "1.2\n",
            "0.006738920230418444  0.006719629745930433\n",
            "0.95\n",
            "0.006808947306126356  0.006790853571146727\n",
            "0.82\n",
            "0.0052961185574531555  0.005285393912345171\n",
            "1.31\n",
            "0.008494737558066845  0.00845377892255783\n",
            "0.99\n",
            "0.00653299642726779  0.006508359685540199\n",
            "0.59\n",
            "0.005439794156700373  0.005420708563178778\n",
            "0.8\n",
            "0.006639121100306511  0.006618714891374111\n",
            "0.57\n",
            "0.003766696434468031  0.0037525773514062166\n",
            "1.02\n",
            "0.0054733785800635815  0.005433179903775454\n",
            "0.95\n",
            "0.007365092635154724  0.0073400079272687435\n",
            "1.08\n",
            "0.006153696682304144  0.00613444996997714\n",
            "0.71\n",
            "0.005626620724797249  0.005588856525719166\n",
            "1.36\n",
            "0.008573394268751144  0.008525500074028969\n",
            "1.35\n",
            "0.0056146676652133465  0.005547458305954933\n",
            "1.22\n",
            "0.0060900431126356125  0.006042046472430229\n",
            "0.99\n",
            "0.005341022741049528  0.005311756394803524\n",
            "1.01\n",
            "0.005914594512432814  0.005868484266102314\n",
            "0.59\n",
            "0.0045747775584459305  0.004576787352561951\n",
            "0.72\n",
            "0.006194202229380608  0.0061660585924983025\n",
            "0.87\n",
            "0.005472336895763874  0.005434583406895399\n",
            "1.18\n",
            "0.005113421939313412  0.005069078411906958\n",
            "0.66\n",
            "0.00419394439086318  0.004159300588071346\n",
            "0.68\n",
            "0.0043551684357225895  0.00432656379416585\n",
            "0.83\n",
            "0.004977460019290447  0.004955878481268883\n",
            "0.83\n",
            "0.006026105489581823  0.0059894067235291\n",
            "0.71\n",
            "0.004671519156545401  0.004632040858268738\n",
            "1.01\n",
            "0.004459348041564226  0.004427577834576368\n",
            "1.15\n",
            "0.007076018024235964  0.007052849046885967\n",
            "0.86\n",
            "0.0045682149939239025  0.004518554080277681\n",
            "1.15\n",
            "0.007127862423658371  0.007065679877996445\n",
            "0.99\n",
            "0.00616092374548316  0.006131940521299839\n",
            "1.14\n",
            "0.0058125960640609264  0.005759194027632475\n",
            "1.28\n",
            "0.0056653860956430435  0.005636555142700672\n",
            "0.91\n",
            "0.004906128626316786  0.00485649611800909\n",
            "1.36\n",
            "0.00643635168671608  0.006388167850673199\n",
            "0.98\n",
            "0.007357713766396046  0.007311939727514982\n",
            "0.9\n",
            "0.004674912896007299  0.004640374332666397\n",
            "0.69\n",
            "0.004183592740446329  0.004152278415858746\n",
            "1.22\n",
            "0.005837619304656982  0.005758730694651604\n",
            "0.86\n",
            "0.0044407970272004604  0.004391738213598728\n",
            "0.72\n",
            "0.0037948002573102713  0.003746034810319543\n",
            "1.0\n",
            "0.006154356058686972  0.006110938731580973\n",
            "0.86\n",
            "0.004451600834727287  0.00441002007573843\n",
            "1.16\n",
            "0.006549131590873003  0.006498628295958042\n",
            "1.38\n",
            "0.0051954747177660465  0.005097563844174147\n",
            "0.96\n",
            "0.0059144096449017525  0.005857804324477911\n",
            "0.89\n",
            "0.004040976520627737  0.003970316145569086\n",
            "1.02\n",
            "0.005501644220203161  0.00540611706674099\n",
            "1.11\n",
            "0.006111196242272854  0.006034643854945898\n",
            "1.07\n",
            "0.004082913976162672  0.003993756137788296\n",
            "0.77\n",
            "0.004789190832525492  0.0047191958874464035\n",
            "0.85\n",
            "0.0041175726801157  0.0040422528982162476\n",
            "0.85\n",
            "0.005339205730706453  0.005264141596853733\n",
            "1.04\n",
            "0.004390731453895569  0.0042829192243516445\n",
            "0.9\n",
            "0.003934571985155344  0.003855334594845772\n",
            "0.87\n",
            "0.004853260703384876  0.0047835190780460835\n",
            "1.29\n",
            "0.005212935619056225  0.005087229888886213\n",
            "1.32\n",
            "0.005226964596658945  0.0050799730233848095\n",
            "0.83\n",
            "0.004379655700176954  0.004305076319724321\n",
            "0.76\n",
            "0.00484122009947896  0.004723039921373129\n",
            "1.33\n",
            "0.004742257762700319  0.0046145124360919\n",
            "0.91\n",
            "0.003972421400249004  0.0038852423895150423\n",
            "0.62\n",
            "0.003688630647957325  0.003609332488849759\n",
            "1.2\n",
            "0.0050580669194459915  0.004922277759760618\n",
            "1.2\n",
            "0.0065181320533156395  0.006382350344210863\n",
            "0.99\n",
            "0.005062358919531107  0.004930470138788223\n",
            "1.02\n",
            "0.004613443277776241  0.004476671107113361\n",
            "0.69\n",
            "0.005075371358543634  0.005029360763728619\n",
            "0.83\n",
            "0.0037295620422810316  0.0036260667257010937\n",
            "0.67\n",
            "0.0048741064965724945  0.004789699800312519\n",
            "0.34\n",
            "0.0047122579999268055  0.004654904827475548\n",
            "0.75\n",
            "0.0036626227665692568  0.003590065985918045\n",
            "1.0\n",
            "0.00441950187087059  0.004298908170312643\n",
            "1.04\n",
            "0.004158270079642534  0.004038238897919655\n",
            "0.94\n",
            "0.004020766355097294  0.0038892761804163456\n",
            "1.06\n",
            "0.005630790255963802  0.005483746994286776\n",
            "0.52\n",
            "0.0022695863153785467  0.00218559755012393\n",
            "1.15\n",
            "0.0060652680695056915  0.005903569515794516\n",
            "1.22\n",
            "0.005728944204747677  0.005559122655540705\n",
            "0.9\n",
            "0.005248151253908873  0.005116662941873074\n",
            "0.75\n",
            "0.004087500739842653  0.004009167663753033\n",
            "0.93\n",
            "0.00432890560477972  0.004197539761662483\n",
            "0.58\n",
            "0.003536833915859461  0.0034400993026793003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn67gJiIjVp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "envi = env(8, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3JqYWZjYsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5376267e-c9da-4b7e-9c85-411b975f8a82"
      },
      "source": [
        "#envi.agents_self_goal_collected\n",
        "#envi.agents_othr_goal_collected\n",
        "envi.agents_nthr_goal_collected"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}